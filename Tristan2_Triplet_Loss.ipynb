{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tristan2_Triplet_Loss.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "SbvuiELfxwNV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "outputId": "4ef2f59e-5d25-4875-85c3-02dd289bd340"
      },
      "cell_type": "code",
      "source": [
        "!pip install keras --upgrade\n",
        "!uninstall ngdlm\n",
        "!pip install git+https://github.com/AI-Guru/ngdlm.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (0.19.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.6)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "/bin/bash: uninstall: command not found\n",
            "Collecting git+https://github.com/AI-Guru/ngdlm.git\n",
            "  Cloning https://github.com/AI-Guru/ngdlm.git to /tmp/pip-req-build-z2vv5674\n",
            "Requirement already satisfied (use --upgrade to upgrade): ngdlm==0.0.2rc1 from git+https://github.com/AI-Guru/ngdlm.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from ngdlm==0.0.2rc1) (2.2.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from ngdlm==0.0.2rc1) (1.12.0rc1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from ngdlm==0.0.2rc1) (2.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->ngdlm==0.0.2rc1) (0.19.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->ngdlm==0.0.2rc1) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->ngdlm==0.0.2rc1) (1.14.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->ngdlm==0.0.2rc1) (1.0.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->ngdlm==0.0.2rc1) (1.11.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->ngdlm==0.0.2rc1) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->ngdlm==0.0.2rc1) (2.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->ngdlm==0.0.2rc1) (3.6.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->ngdlm==0.0.2rc1) (0.7.1)\n",
            "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->ngdlm==0.0.2rc1) (1.11.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->ngdlm==0.0.2rc1) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->ngdlm==0.0.2rc1) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->ngdlm==0.0.2rc1) (0.6.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->ngdlm==0.0.2rc1) (0.32.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->ngdlm==0.0.2rc1) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ngdlm==0.0.2rc1) (2.2.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib->ngdlm==0.0.2rc1) (2018.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ngdlm==0.0.2rc1) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->ngdlm==0.0.2rc1) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->ngdlm==0.0.2rc1) (40.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow->ngdlm==0.0.2rc1) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow->ngdlm==0.0.2rc1) (3.0.1)\n",
            "Building wheels for collected packages: ngdlm\n",
            "  Running setup.py bdist_wheel for ngdlm ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-dxisxxj0/wheels/93/06/27/e156acb49f475c364c3c9fa4ad4ab7bfa38808bff5bf9c4647\n",
            "Successfully built ngdlm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KFRr-p41x2hi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "48e795d9-da40-4595-d20e-67a42649ddb9"
      },
      "cell_type": "code",
      "source": [
        "from keras import models, layers, optimizers\n",
        "from ngdlm import models as ngdlmodels\n",
        "from ngdlm import utils as ngdlutils\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qH51XutHx6-h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "55734dba-39fc-4955-86c9-1376ea34dc3e"
      },
      "cell_type": "code",
      "source": [
        "latent_dim = 8\n",
        "\n",
        "# Base-model.\n",
        "base_input = layers.Input(shape = (28,28))\n",
        "base_output = base_input\n",
        "base_output = layers.Flatten()(base_output)\n",
        "base_output = layers.Dense(512, activation = \"relu\")(base_output)\n",
        "base_output = layers.Dense(256, activation = \"relu\")(base_output)\n",
        "base_output = layers.Dense(128, activation = \"relu\")(base_output)\n",
        "base_output = layers.Dense(latent_dim)(base_output)\n",
        "base = models.Model(base_input, base_output)\n",
        "#base.summary()\n",
        "\n",
        "tl = ngdlmodels.TL(base)\n",
        "tl.summary()\n",
        "\n",
        "tl.compile(optimizer=\"rmsprop\", triplet_loss=\"euclidean\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Basemodel:\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        (None, 28, 28)            0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 8)                 1032      \n",
            "=================================================================\n",
            "Total params: 567,176\n",
            "Trainable params: 567,176\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Siamese model:\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_12 (InputLayer)           (None, 28, 28)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_13 (InputLayer)           (None, 28, 28)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           (None, 28, 28)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_4 (Model)                 (None, 8)            567176      input_12[0][0]                   \n",
            "                                                                 input_13[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 24)           0           model_4[1][0]                    \n",
            "                                                                 model_4[2][0]                    \n",
            "                                                                 model_4[3][0]                    \n",
            "==================================================================================================\n",
            "Total params: 567,176\n",
            "Trainable params: 567,176\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AiLTE3Zrz7QJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_input_train, y_output_train), (x_input_test, y_output_test) = mnist.load_data()\n",
        "x_input_train = x_input_train.astype(\"float32\") / 255.0\n",
        "x_input_test = x_input_test.astype(\"float32\") / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MID5rb8pzzVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14049
        },
        "outputId": "7dc425a8-770d-43fe-a0fd-a3abab6eeb0b"
      },
      "cell_type": "code",
      "source": [
        "history = tl.fit(\n",
        "    x_input_train, y_output_train,\n",
        "    epochs=1000,\n",
        "    batch_size=128,\n",
        "    steps_per_epoch=1000,\n",
        "    minibatch_size = 10,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_input_test, y_output_test),\n",
        "    validation_steps=500\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000...\n",
            "loss [3.393533057533205e-05]\n",
            "val_loss [0.1436370313167572]\n",
            "Epoch 2/1000...\n",
            "loss [0.0001404608408920467]\n",
            "val_loss [0.06959792971611023]\n",
            "Epoch 3/1000...\n",
            "loss [7.663924153894187e-05]\n",
            "val_loss [0.05387656390666962]\n",
            "Epoch 4/1000...\n",
            "loss [4.339488968253136e-05]\n",
            "val_loss [0.07267004251480103]\n",
            "Epoch 5/1000...\n",
            "loss [6.115398765541613e-05]\n",
            "val_loss [0.1482819765806198]\n",
            "Epoch 6/1000...\n",
            "loss [8.763066213577985e-05]\n",
            "val_loss [0.09541218727827072]\n",
            "Epoch 7/1000...\n",
            "loss [6.402366189286112e-05]\n",
            "val_loss [0.10519059002399445]\n",
            "Epoch 8/1000...\n",
            "loss [0.00017971954424865543]\n",
            "val_loss [0.1004280149936676]\n",
            "Epoch 9/1000...\n",
            "loss [6.008002674207091e-05]\n",
            "val_loss [0.06344232708215714]\n",
            "Epoch 10/1000...\n",
            "loss [5.6744952918961645e-05]\n",
            "val_loss [0.13932889699935913]\n",
            "Epoch 11/1000...\n",
            "loss [0.00016154004936106504]\n",
            "val_loss [0.162276029586792]\n",
            "Epoch 12/1000...\n",
            "loss [5.800849199295044e-05]\n",
            "val_loss [0.12107395380735397]\n",
            "Epoch 13/1000...\n",
            "loss [0.00010887983441352844]\n",
            "val_loss [0.08228569477796555]\n",
            "Epoch 14/1000...\n",
            "loss [0.00010374145396053791]\n",
            "val_loss [0.14437173306941986]\n",
            "Epoch 15/1000...\n",
            "loss [7.162695750594139e-05]\n",
            "val_loss [0.1363513171672821]\n",
            "Epoch 16/1000...\n",
            "loss [0.00015613359794951976]\n",
            "val_loss [0.0870182067155838]\n",
            "Epoch 17/1000...\n",
            "loss [0.00015375287947244943]\n",
            "val_loss [0.13260117173194885]\n",
            "Epoch 18/1000...\n",
            "loss [6.337119615636767e-05]\n",
            "val_loss [0.1562047153711319]\n",
            "Epoch 19/1000...\n",
            "loss [0.00011375277489423751]\n",
            "val_loss [0.08461375534534454]\n",
            "Epoch 20/1000...\n",
            "loss [6.543555296957493e-05]\n",
            "val_loss [0.14083142578601837]\n",
            "Epoch 21/1000...\n",
            "loss [4.6801988501101734e-05]\n",
            "val_loss [0.03584669902920723]\n",
            "Epoch 22/1000...\n",
            "loss [8.873532456345856e-05]\n",
            "val_loss [0.11567558348178864]\n",
            "Epoch 23/1000...\n",
            "loss [0.00010623436374589801]\n",
            "val_loss [0.09302433580160141]\n",
            "Epoch 24/1000...\n",
            "loss [4.440415138378739e-05]\n",
            "val_loss [0.15056642889976501]\n",
            "Epoch 25/1000...\n",
            "loss [6.832046993076801e-05]\n",
            "val_loss [0.16465917229652405]\n",
            "Epoch 26/1000...\n",
            "loss [0.00010458603827282786]\n",
            "val_loss [0.11901053786277771]\n",
            "Epoch 27/1000...\n",
            "loss [9.012065175920724e-05]\n",
            "val_loss [0.10373505204916]\n",
            "Epoch 28/1000...\n",
            "loss [0.00011000101640820503]\n",
            "val_loss [0.1405879557132721]\n",
            "Epoch 29/1000...\n",
            "loss [0.00017754113022238016]\n",
            "val_loss [0.15665559470653534]\n",
            "Epoch 30/1000...\n",
            "loss [7.188209518790246e-05]\n",
            "val_loss [0.11348335444927216]\n",
            "Epoch 31/1000...\n",
            "loss [8.489236072637141e-05]\n",
            "val_loss [0.07096171379089355]\n",
            "Epoch 32/1000...\n",
            "loss [0.00010437956778332591]\n",
            "val_loss [0.19235458970069885]\n",
            "Epoch 33/1000...\n",
            "loss [6.583304423838854e-05]\n",
            "val_loss [0.11665493249893188]\n",
            "Epoch 34/1000...\n",
            "loss [7.778180693276226e-05]\n",
            "val_loss [0.10697931796312332]\n",
            "Epoch 35/1000...\n",
            "loss [8.100594393908978e-05]\n",
            "val_loss [0.16751079261302948]\n",
            "Epoch 36/1000...\n",
            "loss [6.596716376952827e-05]\n",
            "val_loss [0.067283034324646]\n",
            "Epoch 37/1000...\n",
            "loss [0.00012467121798545122]\n",
            "val_loss [0.11926256120204926]\n",
            "Epoch 38/1000...\n",
            "loss [0.00011372860451228916]\n",
            "val_loss [0.040366511791944504]\n",
            "Epoch 39/1000...\n",
            "loss [7.580708339810372e-05]\n",
            "val_loss [0.14487063884735107]\n",
            "Epoch 40/1000...\n",
            "loss [9.516583639197052e-05]\n",
            "val_loss [0.07150451838970184]\n",
            "Epoch 41/1000...\n",
            "loss [0.00014750618417747318]\n",
            "val_loss [0.1062849834561348]\n",
            "Epoch 42/1000...\n",
            "loss [0.00010216748458333314]\n",
            "val_loss [0.13436096906661987]\n",
            "Epoch 43/1000...\n",
            "loss [5.520824668928981e-05]\n",
            "val_loss [0.12689588963985443]\n",
            "Epoch 44/1000...\n",
            "loss [0.00013755137473344802]\n",
            "val_loss [0.07570816576480865]\n",
            "Epoch 45/1000...\n",
            "loss [0.00014445901289582253]\n",
            "val_loss [0.1661556512117386]\n",
            "Epoch 46/1000...\n",
            "loss [0.00011939988378435374]\n",
            "val_loss [0.045324720442295074]\n",
            "Epoch 47/1000...\n",
            "loss [6.348640192300082e-05]\n",
            "val_loss [0.0724439024925232]\n",
            "Epoch 48/1000...\n",
            "loss [9.885255619883537e-05]\n",
            "val_loss [0.17661835253238678]\n",
            "Epoch 49/1000...\n",
            "loss [0.00010494434647262097]\n",
            "val_loss [0.15461762249469757]\n",
            "Epoch 50/1000...\n",
            "loss [0.00014118009340018033]\n",
            "val_loss [0.15956833958625793]\n",
            "Epoch 51/1000...\n",
            "loss [0.0001005229582078755]\n",
            "val_loss [0.11673244833946228]\n",
            "Epoch 52/1000...\n",
            "loss [6.191414501518011e-05]\n",
            "val_loss [0.09156792610883713]\n",
            "Epoch 53/1000...\n",
            "loss [0.00011102461069822311]\n",
            "val_loss [0.06919928640127182]\n",
            "Epoch 54/1000...\n",
            "loss [3.835661802440882e-05]\n",
            "val_loss [0.17248307168483734]\n",
            "Epoch 55/1000...\n",
            "loss [0.00013183256424963475]\n",
            "val_loss [0.08277854323387146]\n",
            "Epoch 56/1000...\n",
            "loss [0.00013778564031235874]\n",
            "val_loss [0.12758520245552063]\n",
            "Epoch 57/1000...\n",
            "loss [9.584807138890028e-05]\n",
            "val_loss [0.0835997462272644]\n",
            "Epoch 58/1000...\n",
            "loss [0.00010133998561650515]\n",
            "val_loss [0.1770107001066208]\n",
            "Epoch 59/1000...\n",
            "loss [0.00015153050515800715]\n",
            "val_loss [0.07774914801120758]\n",
            "Epoch 60/1000...\n",
            "loss [0.00012707733176648618]\n",
            "val_loss [0.05415710061788559]\n",
            "Epoch 61/1000...\n",
            "loss [9.248210117220878e-05]\n",
            "val_loss [0.12046325206756592]\n",
            "Epoch 62/1000...\n",
            "loss [0.00010347132943570614]\n",
            "val_loss [0.14326909184455872]\n",
            "Epoch 63/1000...\n",
            "loss [0.00010151232383213937]\n",
            "val_loss [0.12008824199438095]\n",
            "Epoch 64/1000...\n",
            "loss [6.708565657027066e-05]\n",
            "val_loss [0.1260962188243866]\n",
            "Epoch 65/1000...\n",
            "loss [0.0001282066637650132]\n",
            "val_loss [0.07900413870811462]\n",
            "Epoch 66/1000...\n",
            "loss [8.509767986834049e-05]\n",
            "val_loss [0.06564410775899887]\n",
            "Epoch 67/1000...\n",
            "loss [6.27225711941719e-05]\n",
            "val_loss [0.079185850918293]\n",
            "Epoch 68/1000...\n",
            "loss [5.691111646592617e-05]\n",
            "val_loss [0.1734851896762848]\n",
            "Epoch 69/1000...\n",
            "loss [6.073850765824318e-05]\n",
            "val_loss [0.11137665063142776]\n",
            "Epoch 70/1000...\n",
            "loss [4.49582003057003e-05]\n",
            "val_loss [0.06571463495492935]\n",
            "Epoch 71/1000...\n",
            "loss [0.00012069157115183771]\n",
            "val_loss [0.08529912680387497]\n",
            "Epoch 72/1000...\n",
            "loss [0.00014585715113207698]\n",
            "val_loss [0.06729860603809357]\n",
            "Epoch 73/1000...\n",
            "loss [8.21737116202712e-05]\n",
            "val_loss [0.08845806121826172]\n",
            "Epoch 74/1000...\n",
            "loss [9.485877910628915e-05]\n",
            "val_loss [0.14466704428195953]\n",
            "Epoch 75/1000...\n",
            "loss [8.125693351030349e-05]\n",
            "val_loss [0.1098976582288742]\n",
            "Epoch 76/1000...\n",
            "loss [0.00010199866187758744]\n",
            "val_loss [0.10176995396614075]\n",
            "Epoch 77/1000...\n",
            "loss [0.00010359126795083285]\n",
            "val_loss [0.07570087909698486]\n",
            "Epoch 78/1000...\n",
            "loss [8.383781393058598e-05]\n",
            "val_loss [0.18073463439941406]\n",
            "Epoch 79/1000...\n",
            "loss [8.141367859207094e-05]\n",
            "val_loss [0.10923293232917786]\n",
            "Epoch 80/1000...\n",
            "loss [0.00011455224570818245]\n",
            "val_loss [0.05781067907810211]\n",
            "Epoch 81/1000...\n",
            "loss [0.00014623304829001427]\n",
            "val_loss [0.11724339425563812]\n",
            "Epoch 82/1000...\n",
            "loss [0.00010490861372090876]\n",
            "val_loss [0.24437958002090454]\n",
            "Epoch 83/1000...\n",
            "loss [8.197230100631714e-05]\n",
            "val_loss [0.08871378749608994]\n",
            "Epoch 84/1000...\n",
            "loss [0.0001209545109886676]\n",
            "val_loss [0.11390864104032516]\n",
            "Epoch 85/1000...\n",
            "loss [9.447635430842638e-05]\n",
            "val_loss [0.13166023790836334]\n",
            "Epoch 86/1000...\n",
            "loss [6.749643199145794e-05]\n",
            "val_loss [0.13588544726371765]\n",
            "Epoch 87/1000...\n",
            "loss [8.080489980056882e-05]\n",
            "val_loss [0.1314636915922165]\n",
            "Epoch 88/1000...\n",
            "loss [5.7274127611890436e-05]\n",
            "val_loss [0.27083757519721985]\n",
            "Epoch 89/1000...\n",
            "loss [0.00010750119131989778]\n",
            "val_loss [0.15355223417282104]\n",
            "Epoch 90/1000...\n",
            "loss [4.689995036460459e-05]\n",
            "val_loss [0.09589014202356339]\n",
            "Epoch 91/1000...\n",
            "loss [0.00010827761702239514]\n",
            "val_loss [0.09456740319728851]\n",
            "Epoch 92/1000...\n",
            "loss [5.646101851016283e-05]\n",
            "val_loss [0.07224144041538239]\n",
            "Epoch 93/1000...\n",
            "loss [0.00013357683224603535]\n",
            "val_loss [0.0968584343791008]\n",
            "Epoch 94/1000...\n",
            "loss [8.917135396040976e-05]\n",
            "val_loss [0.2078789472579956]\n",
            "Epoch 95/1000...\n",
            "loss [0.00014006447046995162]\n",
            "val_loss [0.11097642779350281]\n",
            "Epoch 96/1000...\n",
            "loss [0.00013919998332858087]\n",
            "val_loss [0.1338476538658142]\n",
            "Epoch 97/1000...\n",
            "loss [7.08703389391303e-05]\n",
            "val_loss [0.13300946354866028]\n",
            "Epoch 98/1000...\n",
            "loss [0.00018179169669747352]\n",
            "val_loss [0.13057124614715576]\n",
            "Epoch 99/1000...\n",
            "loss [0.00010293506924062967]\n",
            "val_loss [0.07380979508161545]\n",
            "Epoch 100/1000...\n",
            "loss [9.059518482536078e-05]\n",
            "val_loss [0.17572134733200073]\n",
            "Epoch 101/1000...\n",
            "loss [9.86238420009613e-05]\n",
            "val_loss [0.04133303090929985]\n",
            "Epoch 102/1000...\n",
            "loss [4.8994301818311214e-05]\n",
            "val_loss [0.043500788509845734]\n",
            "Epoch 103/1000...\n",
            "loss [0.00012436456419527531]\n",
            "val_loss [0.08172304183244705]\n",
            "Epoch 104/1000...\n",
            "loss [9.293413767591119e-05]\n",
            "val_loss [0.08814728260040283]\n",
            "Epoch 105/1000...\n",
            "loss [9.660708205774426e-05]\n",
            "val_loss [0.13022509217262268]\n",
            "Epoch 106/1000...\n",
            "loss [9.642799571156502e-05]\n",
            "val_loss [0.14963200688362122]\n",
            "Epoch 107/1000...\n",
            "loss [6.220189854502678e-05]\n",
            "val_loss [0.12971116602420807]\n",
            "Epoch 108/1000...\n",
            "loss [4.9610011279582975e-05]\n",
            "val_loss [0.09861540049314499]\n",
            "Epoch 109/1000...\n",
            "loss [9.547537099570036e-05]\n",
            "val_loss [0.09114892780780792]\n",
            "Epoch 110/1000...\n",
            "loss [9.900801442563534e-05]\n",
            "val_loss [0.12918885052204132]\n",
            "Epoch 111/1000...\n",
            "loss [9.402260323986411e-05]\n",
            "val_loss [0.05704955384135246]\n",
            "Epoch 112/1000...\n",
            "loss [0.00010501337796449661]\n",
            "val_loss [0.1360030472278595]\n",
            "Epoch 113/1000...\n",
            "loss [0.00015333247045055032]\n",
            "val_loss [0.110344797372818]\n",
            "Epoch 114/1000...\n",
            "loss [7.935574254952372e-05]\n",
            "val_loss [0.060070332139730453]\n",
            "Epoch 115/1000...\n",
            "loss [9.082182496786117e-05]\n",
            "val_loss [0.10264640301465988]\n",
            "Epoch 116/1000...\n",
            "loss [6.658219825476408e-05]\n",
            "val_loss [0.09991835057735443]\n",
            "Epoch 117/1000...\n",
            "loss [0.00013892528065480292]\n",
            "val_loss [0.03063579462468624]\n",
            "Epoch 118/1000...\n",
            "loss [9.647748805582524e-05]\n",
            "val_loss [0.10363688319921494]\n",
            "Epoch 119/1000...\n",
            "loss [5.8808511355891824e-05]\n",
            "val_loss [0.08350027352571487]\n",
            "Epoch 120/1000...\n",
            "loss [0.0001408275356516242]\n",
            "val_loss [0.10086040943861008]\n",
            "Epoch 121/1000...\n",
            "loss [3.554079052992165e-05]\n",
            "val_loss [0.11050599813461304]\n",
            "Epoch 122/1000...\n",
            "loss [9.724026848562061e-05]\n",
            "val_loss [0.09604381769895554]\n",
            "Epoch 123/1000...\n",
            "loss [8.075146481860429e-05]\n",
            "val_loss [0.062174711376428604]\n",
            "Epoch 124/1000...\n",
            "loss [6.263735238462687e-05]\n",
            "val_loss [0.06277544796466827]\n",
            "Epoch 125/1000...\n",
            "loss [6.801976542919874e-05]\n",
            "val_loss [0.10241560637950897]\n",
            "Epoch 126/1000...\n",
            "loss [4.1338808368891475e-05]\n",
            "val_loss [0.08622081577777863]\n",
            "Epoch 127/1000...\n",
            "loss [9.518979745917022e-05]\n",
            "val_loss [0.09962359070777893]\n",
            "Epoch 128/1000...\n",
            "loss [5.739564378745854e-05]\n",
            "val_loss [0.1003054603934288]\n",
            "Epoch 129/1000...\n",
            "loss [0.00016206843708641827]\n",
            "val_loss [0.12784753739833832]\n",
            "Epoch 130/1000...\n",
            "loss [8.005500165745616e-05]\n",
            "val_loss [0.09082630276679993]\n",
            "Epoch 131/1000...\n",
            "loss [0.00010956144612282514]\n",
            "val_loss [0.08063225448131561]\n",
            "Epoch 132/1000...\n",
            "loss [2.7305901981890203e-05]\n",
            "val_loss [0.08621759712696075]\n",
            "Epoch 133/1000...\n",
            "loss [4.874110058881342e-05]\n",
            "val_loss [0.07064562290906906]\n",
            "Epoch 134/1000...\n",
            "loss [1.709655113518238e-05]\n",
            "val_loss [0.045872122049331665]\n",
            "Epoch 135/1000...\n",
            "loss [8.340466395020485e-05]\n",
            "val_loss [0.17698699235916138]\n",
            "Epoch 136/1000...\n",
            "loss [0.00015022639581002296]\n",
            "val_loss [0.14192600548267365]\n",
            "Epoch 137/1000...\n",
            "loss [0.00011397794843651354]\n",
            "val_loss [0.10946010798215866]\n",
            "Epoch 138/1000...\n",
            "loss [3.6044240230694416e-05]\n",
            "val_loss [0.11127376556396484]\n",
            "Epoch 139/1000...\n",
            "loss [0.00015814884100109338]\n",
            "val_loss [0.06586664915084839]\n",
            "Epoch 140/1000...\n",
            "loss [9.672722546383739e-05]\n",
            "val_loss [0.05740518867969513]\n",
            "Epoch 141/1000...\n",
            "loss [9.197394223883749e-05]\n",
            "val_loss [0.15002694725990295]\n",
            "Epoch 142/1000...\n",
            "loss [0.00013511926401406525]\n",
            "val_loss [0.2080952376127243]\n",
            "Epoch 143/1000...\n",
            "loss [4.532603733241558e-05]\n",
            "val_loss [0.05738527327775955]\n",
            "Epoch 144/1000...\n",
            "loss [5.712151457555592e-05]\n",
            "val_loss [0.07443219423294067]\n",
            "Epoch 145/1000...\n",
            "loss [8.338523143902421e-05]\n",
            "val_loss [0.189910888671875]\n",
            "Epoch 146/1000...\n",
            "loss [0.00010789438569918275]\n",
            "val_loss [0.09601838886737823]\n",
            "Epoch 147/1000...\n",
            "loss [5.20706158131361e-05]\n",
            "val_loss [0.03731519728899002]\n",
            "Epoch 148/1000...\n",
            "loss [0.00011233744444325567]\n",
            "val_loss [0.09889846295118332]\n",
            "Epoch 149/1000...\n",
            "loss [9.632259840145707e-05]\n",
            "val_loss [0.08712488412857056]\n",
            "Epoch 150/1000...\n",
            "loss [7.949981279671193e-05]\n",
            "val_loss [0.12057168781757355]\n",
            "Epoch 151/1000...\n",
            "loss [5.292702210135758e-05]\n",
            "val_loss [0.16374552249908447]\n",
            "Epoch 152/1000...\n",
            "loss [0.00010981449880637228]\n",
            "val_loss [0.18544864654541016]\n",
            "Epoch 153/1000...\n",
            "loss [0.0001140827238559723]\n",
            "val_loss [0.16635245084762573]\n",
            "Epoch 154/1000...\n",
            "loss [0.00010727632441557944]\n",
            "val_loss [0.07492436468601227]\n",
            "Epoch 155/1000...\n",
            "loss [0.00010536448657512664]\n",
            "val_loss [0.1120988130569458]\n",
            "Epoch 156/1000...\n",
            "loss [8.565796632319689e-05]\n",
            "val_loss [0.1299901008605957]\n",
            "Epoch 157/1000...\n",
            "loss [6.886947900056838e-05]\n",
            "val_loss [0.15055564045906067]\n",
            "Epoch 158/1000...\n",
            "loss [7.523469300940632e-05]\n",
            "val_loss [0.14369258284568787]\n",
            "Epoch 159/1000...\n",
            "loss [7.35752210021019e-05]\n",
            "val_loss [0.09891553223133087]\n",
            "Epoch 160/1000...\n",
            "loss [9.508936130441725e-05]\n",
            "val_loss [0.08391809463500977]\n",
            "Epoch 161/1000...\n",
            "loss [0.00010240304516628384]\n",
            "val_loss [0.10491511970758438]\n",
            "Epoch 162/1000...\n",
            "loss [9.484395268373191e-05]\n",
            "val_loss [0.0864572525024414]\n",
            "Epoch 163/1000...\n",
            "loss [4.308552201837301e-05]\n",
            "val_loss [0.052000246942043304]\n",
            "Epoch 164/1000...\n",
            "loss [0.0001720615061931312]\n",
            "val_loss [0.13779217004776]\n",
            "Epoch 165/1000...\n",
            "loss [9.577624429948628e-05]\n",
            "val_loss [0.10502944886684418]\n",
            "Epoch 166/1000...\n",
            "loss [3.351287310943007e-05]\n",
            "val_loss [0.2179705649614334]\n",
            "Epoch 167/1000...\n",
            "loss [5.147695448249579e-05]\n",
            "val_loss [0.055853649973869324]\n",
            "Epoch 168/1000...\n",
            "loss [0.00011227311776019633]\n",
            "val_loss [0.13286921381950378]\n",
            "Epoch 169/1000...\n",
            "loss [0.00010401659831404685]\n",
            "val_loss [0.13145706057548523]\n",
            "Epoch 170/1000...\n",
            "loss [0.00010439419187605381]\n",
            "val_loss [0.13167953491210938]\n",
            "Epoch 171/1000...\n",
            "loss [0.0001247490793466568]\n",
            "val_loss [0.11381492763757706]\n",
            "Epoch 172/1000...\n",
            "loss [0.00010201111063361168]\n",
            "val_loss [0.09969871491193771]\n",
            "Epoch 173/1000...\n",
            "loss [4.8830786719918254e-05]\n",
            "val_loss [0.06154787540435791]\n",
            "Epoch 174/1000...\n",
            "loss [7.20904206391424e-05]\n",
            "val_loss [0.13329291343688965]\n",
            "Epoch 175/1000...\n",
            "loss [0.0001337716793641448]\n",
            "val_loss [0.15367759764194489]\n",
            "Epoch 176/1000...\n",
            "loss [7.429586071521044e-05]\n",
            "val_loss [0.12375135719776154]\n",
            "Epoch 177/1000...\n",
            "loss [6.221694150008261e-05]\n",
            "val_loss [0.125022754073143]\n",
            "Epoch 178/1000...\n",
            "loss [8.565794606693089e-05]\n",
            "val_loss [0.06095316261053085]\n",
            "Epoch 179/1000...\n",
            "loss [0.00011851442977786064]\n",
            "val_loss [0.11215508729219437]\n",
            "Epoch 180/1000...\n",
            "loss [7.552757393568755e-05]\n",
            "val_loss [0.10276282578706741]\n",
            "Epoch 181/1000...\n",
            "loss [4.685166082344949e-05]\n",
            "val_loss [0.1358928382396698]\n",
            "Epoch 182/1000...\n",
            "loss [7.22384904511273e-05]\n",
            "val_loss [0.13167202472686768]\n",
            "Epoch 183/1000...\n",
            "loss [0.00011690319329500198]\n",
            "val_loss [0.1621728390455246]\n",
            "Epoch 184/1000...\n",
            "loss [6.15629751700908e-05]\n",
            "val_loss [0.07850949466228485]\n",
            "Epoch 185/1000...\n",
            "loss [8.679907768964768e-05]\n",
            "val_loss [0.09842094779014587]\n",
            "Epoch 186/1000...\n",
            "loss [7.807950465939939e-05]\n",
            "val_loss [0.12008603662252426]\n",
            "Epoch 187/1000...\n",
            "loss [4.54623019322753e-05]\n",
            "val_loss [0.10985800623893738]\n",
            "Epoch 188/1000...\n",
            "loss [9.705784847028553e-05]\n",
            "val_loss [0.2396754026412964]\n",
            "Epoch 189/1000...\n",
            "loss [8.277264493517578e-05]\n",
            "val_loss [0.07238931953907013]\n",
            "Epoch 190/1000...\n",
            "loss [3.837053757160902e-05]\n",
            "val_loss [0.054635003209114075]\n",
            "Epoch 191/1000...\n",
            "loss [4.4988834299147126e-05]\n",
            "val_loss [0.17178046703338623]\n",
            "Epoch 192/1000...\n",
            "loss [9.126314101740718e-05]\n",
            "val_loss [0.12678134441375732]\n",
            "Epoch 193/1000...\n",
            "loss [6.782878562808037e-05]\n",
            "val_loss [0.1717270463705063]\n",
            "Epoch 194/1000...\n",
            "loss [8.906935132108629e-05]\n",
            "val_loss [0.10313726961612701]\n",
            "Epoch 195/1000...\n",
            "loss [0.00013433213625103235]\n",
            "val_loss [0.08688984811306]\n",
            "Epoch 196/1000...\n",
            "loss [0.00012371309124864637]\n",
            "val_loss [0.12793231010437012]\n",
            "Epoch 197/1000...\n",
            "loss [0.00010897228075191378]\n",
            "val_loss [0.17855656147003174]\n",
            "Epoch 198/1000...\n",
            "loss [6.635377579368651e-05]\n",
            "val_loss [0.0971534252166748]\n",
            "Epoch 199/1000...\n",
            "loss [0.00011607442051172257]\n",
            "val_loss [0.18052826821804047]\n",
            "Epoch 200/1000...\n",
            "loss [0.00012573694973252715]\n",
            "val_loss [0.13492648303508759]\n",
            "Epoch 201/1000...\n",
            "loss [3.930222126655281e-05]\n",
            "val_loss [0.08397264033555984]\n",
            "Epoch 202/1000...\n",
            "loss [0.00010186600289307535]\n",
            "val_loss [0.0605890117585659]\n",
            "Epoch 203/1000...\n",
            "loss [5.1344040781259536e-05]\n",
            "val_loss [0.1308324933052063]\n",
            "Epoch 204/1000...\n",
            "loss [6.67684213258326e-05]\n",
            "val_loss [0.10482415556907654]\n",
            "Epoch 205/1000...\n",
            "loss [1.8406120361760258e-05]\n",
            "val_loss [0.11395974457263947]\n",
            "Epoch 206/1000...\n",
            "loss [5.8316496200859544e-05]\n",
            "val_loss [0.14159394800662994]\n",
            "Epoch 207/1000...\n",
            "loss [9.788326849229633e-05]\n",
            "val_loss [0.06736283004283905]\n",
            "Epoch 208/1000...\n",
            "loss [3.5471511539071796e-05]\n",
            "val_loss [0.10707387328147888]\n",
            "Epoch 209/1000...\n",
            "loss [7.954925764352084e-05]\n",
            "val_loss [0.12079647928476334]\n",
            "Epoch 210/1000...\n",
            "loss [0.0001446519624441862]\n",
            "val_loss [0.052753254771232605]\n",
            "Epoch 211/1000...\n",
            "loss [7.287054415792227e-05]\n",
            "val_loss [0.12614063918590546]\n",
            "Epoch 212/1000...\n",
            "loss [0.00010837197583168745]\n",
            "val_loss [0.09778410196304321]\n",
            "Epoch 213/1000...\n",
            "loss [7.578786229714752e-05]\n",
            "val_loss [0.17925560474395752]\n",
            "Epoch 214/1000...\n",
            "loss [7.158851996064186e-05]\n",
            "val_loss [0.058296240866184235]\n",
            "Epoch 215/1000...\n",
            "loss [6.94804412778467e-05]\n",
            "val_loss [0.14460060000419617]\n",
            "Epoch 216/1000...\n",
            "loss [0.00010967099596746266]\n",
            "val_loss [0.11213050782680511]\n",
            "Epoch 217/1000...\n",
            "loss [0.00012110049766488374]\n",
            "val_loss [0.056619711220264435]\n",
            "Epoch 218/1000...\n",
            "loss [4.9784904345870016e-05]\n",
            "val_loss [0.05577976629137993]\n",
            "Epoch 219/1000...\n",
            "loss [0.0001995625051204115]\n",
            "val_loss [0.07108892500400543]\n",
            "Epoch 220/1000...\n",
            "loss [0.00010140564246103168]\n",
            "val_loss [0.11608690023422241]\n",
            "Epoch 221/1000...\n",
            "loss [4.179726215079427e-05]\n",
            "val_loss [0.07459095865488052]\n",
            "Epoch 222/1000...\n",
            "loss [9.518447541631758e-05]\n",
            "val_loss [0.08448601514101028]\n",
            "Epoch 223/1000...\n",
            "loss [0.00011838076845742762]\n",
            "val_loss [0.18026301264762878]\n",
            "Epoch 224/1000...\n",
            "loss [8.620521402917803e-05]\n",
            "val_loss [0.04445637762546539]\n",
            "Epoch 225/1000...\n",
            "loss [5.8575939619913696e-05]\n",
            "val_loss [0.07526487112045288]\n",
            "Epoch 226/1000...\n",
            "loss [7.65353178139776e-05]\n",
            "val_loss [0.07219483703374863]\n",
            "Epoch 227/1000...\n",
            "loss [6.364809023216367e-05]\n",
            "val_loss [0.10998008400201797]\n",
            "Epoch 228/1000...\n",
            "loss [3.350959159433842e-05]\n",
            "val_loss [0.09815920889377594]\n",
            "Epoch 229/1000...\n",
            "loss [6.129992473870516e-05]\n",
            "val_loss [0.0758211687207222]\n",
            "Epoch 230/1000...\n",
            "loss [7.032433641143143e-05]\n",
            "val_loss [0.22467684745788574]\n",
            "Epoch 231/1000...\n",
            "loss [8.985456824302673e-05]\n",
            "val_loss [0.18499265611171722]\n",
            "Epoch 232/1000...\n",
            "loss [0.0001098141148686409]\n",
            "val_loss [0.06030464172363281]\n",
            "Epoch 233/1000...\n",
            "loss [6.10927832312882e-05]\n",
            "val_loss [0.14274808764457703]\n",
            "Epoch 234/1000...\n",
            "loss [0.0001037506649736315]\n",
            "val_loss [0.10989560931921005]\n",
            "Epoch 235/1000...\n",
            "loss [0.00010286803543567657]\n",
            "val_loss [0.12305380403995514]\n",
            "Epoch 236/1000...\n",
            "loss [6.583025958389043e-05]\n",
            "val_loss [0.07284696400165558]\n",
            "Epoch 237/1000...\n",
            "loss [8.65103886462748e-05]\n",
            "val_loss [0.14729681611061096]\n",
            "Epoch 238/1000...\n",
            "loss [7.311357441358269e-05]\n",
            "val_loss [0.14971758425235748]\n",
            "Epoch 239/1000...\n",
            "loss [8.181416336447001e-05]\n",
            "val_loss [0.09006272256374359]\n",
            "Epoch 240/1000...\n",
            "loss [7.77069756295532e-05]\n",
            "val_loss [0.17282146215438843]\n",
            "Epoch 241/1000...\n",
            "loss [4.9029229208827016e-05]\n",
            "val_loss [0.17021146416664124]\n",
            "Epoch 242/1000...\n",
            "loss [0.0001059384299442172]\n",
            "val_loss [0.15431928634643555]\n",
            "Epoch 243/1000...\n",
            "loss [0.00011664832383394241]\n",
            "val_loss [0.09397324919700623]\n",
            "Epoch 244/1000...\n",
            "loss [8.724622847512364e-05]\n",
            "val_loss [0.10561897605657578]\n",
            "Epoch 245/1000...\n",
            "loss [7.523600570857525e-05]\n",
            "val_loss [0.12875676155090332]\n",
            "Epoch 246/1000...\n",
            "loss [5.690610641613603e-05]\n",
            "val_loss [0.07530279457569122]\n",
            "Epoch 247/1000...\n",
            "loss [7.480485667474568e-05]\n",
            "val_loss [0.03789868205785751]\n",
            "Epoch 248/1000...\n",
            "loss [7.338991551660002e-05]\n",
            "val_loss [0.12098526954650879]\n",
            "Epoch 249/1000...\n",
            "loss [0.00015082628466188907]\n",
            "val_loss [0.10044680535793304]\n",
            "Epoch 250/1000...\n",
            "loss [9.960462409071624e-05]\n",
            "val_loss [0.05537960305809975]\n",
            "Epoch 251/1000...\n",
            "loss [6.67457461822778e-05]\n",
            "val_loss [0.04035454988479614]\n",
            "Epoch 252/1000...\n",
            "loss [8.278445969335735e-05]\n",
            "val_loss [0.09983653575181961]\n",
            "Epoch 253/1000...\n",
            "loss [3.561458317562938e-05]\n",
            "val_loss [0.14778408408164978]\n",
            "Epoch 254/1000...\n",
            "loss [0.00013694089255295695]\n",
            "val_loss [0.04831577092409134]\n",
            "Epoch 255/1000...\n",
            "loss [3.777371346950531e-05]\n",
            "val_loss [0.023916808888316154]\n",
            "Epoch 256/1000...\n",
            "loss [7.714392081834376e-05]\n",
            "val_loss [0.1371617615222931]\n",
            "Epoch 257/1000...\n",
            "loss [7.822196395136416e-05]\n",
            "val_loss [0.05166182294487953]\n",
            "Epoch 258/1000...\n",
            "loss [6.89512388780713e-05]\n",
            "val_loss [0.04497058689594269]\n",
            "Epoch 259/1000...\n",
            "loss [3.4175674431025984e-05]\n",
            "val_loss [0.20869363844394684]\n",
            "Epoch 260/1000...\n",
            "loss [5.8381867595016956e-05]\n",
            "val_loss [0.08750256896018982]\n",
            "Epoch 261/1000...\n",
            "loss [9.689932176843286e-05]\n",
            "val_loss [0.06208931654691696]\n",
            "Epoch 262/1000...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e27ef6df2f79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_output_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ngdlm/models/tl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, minibatch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m             )\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}